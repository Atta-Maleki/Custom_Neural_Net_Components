{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lIYdn1woOS1n"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["class Model():\n","  def __init__(self):\n","    self.w = tf.Variable(2.0)\n","    self.b = tf.Variable(1.0)\n","  \n","  def __call__(self, x):\n","    return self.w * x + self.b\n"],"metadata":{"id":"ctQ6TNFfPjEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["real_w = 3.0\n","real_b = 2.0\n","numExamples = 1000\n","\n","random_xs = tf.random.normal(shape = [numExamples])\n","\n","real_ys = real_w * random_xs + real_b\n","#data is ready now."],"metadata":{"id":"SHYzjBQ6RF2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def myLoss (y_real, y_pred):\n","  loss = tf.reduce_mean(tf.square(y_real - y_pred))\n","  return loss"],"metadata":{"id":"EP0IBqn_Rijf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, inputs, outputs, learning_rate):\n","  with tf.GradientTape() as tape:\n","    currentLoss = myLoss(outputs, model(inputs))\n","  dw , db =  tape.gradient(currentLoss, [model.w , model.b])\n","  \n","  model.w.assign_sub(learning_rate * dw)\n","  model.b.assign_sub(learning_rate * db)\n","  return currentLoss"],"metadata":{"id":"4yD8gNUySmFn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numEpochs = 25\n","list_w , list_b = [], []\n","list_Loss = []\n","model = Model()\n","for epoch in range(numEpochs):\n","  list_w.append(model.w.numpy())\n","  list_b.append(model.b.numpy())\n","  current_loss = train(model, random_xs, real_ys, learning_rate = 0.1)\n","  list_Loss.append(current_loss)\n","  print('Epoch: %2d : w = %1.2f b = %1.2f Loss =%2.5f' %\n","        (epoch, list_w[-1], list_b[-1], current_loss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KD_n5abvT853","executionInfo":{"status":"ok","timestamp":1683129706391,"user_tz":-210,"elapsed":15,"user":{"displayName":"Atta Maleki","userId":"09673812612104214910"}},"outputId":"3b59e56e-ce52-40db-ef68-29299db3e381"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:  0 : w = 2.00 b = 1.00 Loss =2.14405\n","Epoch:  1 : w = 2.22 b = 1.21 Loss =1.32246\n","Epoch:  2 : w = 2.39 b = 1.37 Loss =0.81598\n","Epoch:  3 : w = 2.53 b = 1.50 Loss =0.50365\n","Epoch:  4 : w = 2.63 b = 1.60 Loss =0.31098\n","Epoch:  5 : w = 2.72 b = 1.69 Loss =0.19209\n","Epoch:  6 : w = 2.78 b = 1.75 Loss =0.11870\n","Epoch:  7 : w = 2.83 b = 1.80 Loss =0.07338\n","Epoch:  8 : w = 2.87 b = 1.84 Loss =0.04538\n","Epoch:  9 : w = 2.90 b = 1.88 Loss =0.02808\n","Epoch: 10 : w = 2.92 b = 1.90 Loss =0.01738\n","Epoch: 11 : w = 2.94 b = 1.92 Loss =0.01076\n","Epoch: 12 : w = 2.95 b = 1.94 Loss =0.00667\n","Epoch: 13 : w = 2.96 b = 1.95 Loss =0.00413\n","Epoch: 14 : w = 2.97 b = 1.96 Loss =0.00256\n","Epoch: 15 : w = 2.98 b = 1.97 Loss =0.00159\n","Epoch: 16 : w = 2.98 b = 1.98 Loss =0.00099\n","Epoch: 17 : w = 2.99 b = 1.98 Loss =0.00061\n","Epoch: 18 : w = 2.99 b = 1.98 Loss =0.00038\n","Epoch: 19 : w = 2.99 b = 1.99 Loss =0.00024\n","Epoch: 20 : w = 2.99 b = 1.99 Loss =0.00015\n","Epoch: 21 : w = 3.00 b = 1.99 Loss =0.00009\n","Epoch: 22 : w = 3.00 b = 1.99 Loss =0.00006\n","Epoch: 23 : w = 3.00 b = 1.99 Loss =0.00004\n","Epoch: 24 : w = 3.00 b = 2.00 Loss =0.00002\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Oee0KOqxVJAx"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"/v2/external/notebooks/empty.ipynb","timestamp":1683128109078}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}